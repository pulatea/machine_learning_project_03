{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb933eab",
   "metadata": {},
   "source": [
    "# Home Assignment No. 3: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e21e01",
   "metadata": {},
   "source": [
    "In this part of the homework you need to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* Please include your name, surname and matriculation number in the beginning of the notebook file.\n",
    "\n",
    "* Please contact Alp Eren SARI (alp.sari@unibe.ch) for questions and problems related to the assignment\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must be **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141d88f",
   "metadata": {},
   "source": [
    "## Task 1. Ensembling Method [4 points]\n",
    "\n",
    "Suppose that we have random variables $X_i$ for $0 \\leq i \\leq n$ with $Var(X_i) = \\sigma_i^2$. Moreover, any $X_k$ and $X_l$ are correlated by a factor of ${\\rho}_{kl}$ for any k and l. Calculate the variance of the average of these random variables as in $Z = \\frac{1}{n}\\sum\\limits_{i=0}^{n}X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700866c",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your Solution: }}$\n",
    "\n",
    "First, we want to calculate the variance of the average $\\hat{\\varepsilon}$:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\text{VAR}(\\hat{\\varepsilon}) &= E\\left[\\frac{1}{k} \\sum_{i=1}^{n} (\\varepsilon_k - \\mu_k) \\cdot \\frac{1}{k} \\sum_{j=1}^{n} (\\varepsilon_l - \\mu_l)\\right] \\\\\n",
    "&= \\frac{1}{k^2} \\left( E\\left[ \\sum_{k=1}^{N} (\\varepsilon_k - \\mu_k)^2 + \\sum_{\\substack{k \\neq l}}^{N} (\\varepsilon_k - \\mu_k)(\\varepsilon_l - \\mu_l)^{\\intercal} \\right] \\right)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "If $X_k$ and $X_l$ are correlated by a factor of ${\\rho}_{kl}$, then we consider the correlation factor equation below:\n",
    "\n",
    "$\n",
    "\\rho_{kl} = \\frac{{E\\left[(\\varepsilon_k - \\mu_k)(\\varepsilon_l - \\mu_l)\\right]}}{{\\sigma^2}}\n",
    "$\n",
    "\n",
    "We should also consider that by definition $ (\\varepsilon - \\mu)^2 = \\sigma^2 $ -> $E\\left[\\sum_{k=1}^{K} \\sigma^2\\right] = k \\sigma^2$ where we sum $\\sigma^2$ k-times.\n",
    "\n",
    "We plug these two equations into our main variance equation and we get:\n",
    "\n",
    "$\n",
    "\\text{VAR}(\\hat{\\varepsilon}) = \\frac{1}{k^2} \\left(k \\sigma^2 + \\sum_{\\substack{k \\neq l}} \\rho \\sigma^2 \\right) = \\frac{k \\sigma^2}{k^2} + \\frac{(k^2 - k) \\rho \\sigma^2}{k^2} = \\frac{\\sigma^2}{k} + \\frac{k(k-1) \\rho \\sigma^2}{k^2} = \\frac{\\sigma^2}{k} + \\frac{(k-1) \\rho \\sigma^2}{k}\n",
    "$\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8777db9",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Minimizing the loss function is an optimization task, and **\"Gradient Boosting\" is one of many methods to perform optimization**. It uses a greedy approach and, therefore, may produce results that are not _globally_ optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & G_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\alpha_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & f_N(x) = \\sum_{n=0}^N \\alpha_n (x) G_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss function $\\mathcal{L}(y, \\hat{y})$ for a target $y$ and a prediction $\\hat{y}$, and let\n",
    "$(x_i, y_i)_{i = 1}^m$ be our train dataset for the regression task. \n",
    "\n",
    "\n",
    "1. Initialize $f_0(x) = z$ with the _flat constant prediction_\n",
    "\n",
    "$$z = \\arg\\min\\limits_{\\hat{y} \\in \\mathbb{R}} \\sum\\limits_{i = 1}^m \\mathcal{L}(y_i, \\hat{y})$$\n",
    "\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $L_n(G_n, \\alpha_n) \\to \\min\\limits_{G_{n}, \\alpha_n}$, where\n",
    "    \n",
    "    $$ L_n(G, \\alpha) = \\sum\\limits_{i = 1}^m \\mathcal{L}\\bigl(y_i, f_{n - 1}(x_i) + \\alpha G(x_i)\\bigr) $$\n",
    "    \n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & g_i = \\left. -\\frac{\\partial \\mathcal{L}(y_i, \\hat{y})}{\\partial \\hat{y}} \\right|_{\\hat{y} = G_{n - 1}(x_i)}\n",
    "          \\\\\n",
    "      & G_n(x) = \\arg\\min\\limits_{G \\in \\mathcal{A}}\\sum\\limits_{i = 1}^l \\bigl(G(x_i) - g_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\alpha_n = \\arg\\min\\limits_\\alpha L_n(G_n, \\alpha)\n",
    "          \\\\\n",
    "      & f_n(x) = f_{n - 1}(x) + \\alpha_n G_n(x)\n",
    "    \\end{align}\n",
    "    \n",
    "3. return $f_N(x) = f_0(x) + \\sum\\limits_{n = 1}^N \\alpha_n G_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96866b0",
   "metadata": {},
   "source": [
    "## Task 2. Gradient Boosting. [6 points]\n",
    "\n",
    "Assume that we've already found the optimal $G_n(x)$ at the step $n$ and we already have $f_{n-1}(x)$ from the previous step. Derive the expression for the **optimal value** of $\\alpha_n$ for the _MSE_ loss function $\\mathcal{L}(y, \\hat{y}) = (y - \\hat{y})^2$. Furthermore, explain what would happen to $\\alpha_n$ if the $|y_i - f_{n-1}(x_i)|$ values are close to 0 or significantly greater than $G_n(x_i)$ where $x_i$'s are data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a1569f8e960c",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "If we have samples $(x_i, y_i)$ for i = 1, ..., m and have the loss function $\\mathcal{L}(y, \\hat{y})$.\n",
    "\n",
    "The expanded loss function for $L_n(G_n, \\alpha_n)$ is as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "    L_n(G_n, \\alpha_n) &= \\sum_{i=1}^m (y_i - f_{n-1}(x_i) - \\alpha_n G_n(x_i))^2 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The value that we should optimise is $\\alpha_n$, so we find the derivative of our loss function w.r.t $\\alpha_n$, and we get:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\alpha_n}{\\partial L_n(G_n, \\alpha_n)} = -2 \\sum_{i=1}^m (y_i - f_{n-1}(x_i))G_n(x_i) = -2 \\sum_{i=1}^m ((y_i - f_{n-1}(x_i))G_n(x_i) - \\alpha_n (G_n(x_i))^2) = 2\\alpha_n \\sum_{i=1}^m (G_n(x_i))^2 - 2 \\sum_{i=1}^m (y_i - f_{n-1}(x_i))G_n(x_i)\n",
    "$\n",
    "\n",
    "\n",
    "After finding the derivative, we should set it to 0 and solve it for $\\alpha_n$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    & -2 \\sum_{i=1}^m (y_i - f_{n-1}(x_i))G_n(x_i) + 2 \\alpha_n \\sum_{i=1}^m (G_n(x_i))^2 = 0 \\\\\n",
    "    & \\alpha_n \\sum_{i=1}^m (G_n(x_i))^2 = \\sum_{i=1}^m (y_i - f_{n-1}(x_i))G_n(x_i) \\\\\n",
    "    & \\alpha_n = \\frac{\\sum_{i=1}^m (y_i - f_{n-1}(x_i))G_n(x_i)}{\\sum_{i=1}^m (G_n(x_i))^2}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "If the difference between what our model predicted $f_{n-1}(x_i)$, and the actual outcome $y_i$ is very small, it means that the model is finding the data pattern correctly. So, it means it does not have a high weight, since $\\alpha_n$ is greater when the importance of misclassification is high. Our $\\alpha_n$ value in this case is small.\n",
    "In the other hand, if the difference is a large number, it means that the model is weak and has not classified the sample correctly, so $\\alpha_n$ should be greater in value, emphasizing the importance of the error so in the next iteration it will pay attention to that misclassified data sample.\n",
    "\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9b45b",
   "metadata": {},
   "source": [
    "\n",
    "We want to find the optimal value of $\\alpha_n$ for the boosting step at iteration n. The optimization problem is as follows:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    & G_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\alpha_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & f_N(x) = \\sum_{n=0}^N \\alpha_n (x) G_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3086f",
   "metadata": {},
   "source": [
    "## Task 3. AdaBoost Weight Updates [3 points]\n",
    "\n",
    "$\\alpha_m$ parameter in AdaBoost update goes to infinity when ${err}_m$ goes to 0. What are the implications of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fea010",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "-----------------\n",
    "\n",
    "Adaptive Boosting is a method with exponential loss for classification. The exponential loss used in AdaBoost encourages the model to focus on difficult-to-classify examples. In each iteration, it decreases the training error, so in the long run, it will be pushed to zero. When the error goes to zero, the weights parameter goes to infinity. When α goes to infinity, that means that our classifier at the particular iteration has very high influence in the final decision, which means that we might have correctly classified all our samples, but the chance is high that we have also overfitted noise and outliers in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7459999",
   "metadata": {},
   "source": [
    "## Task 4. Expectation Maximization (EM) [4 points]\n",
    "\n",
    "Prove that the following equation hold\n",
    "\n",
    "$max(\\sum\\limits_{i}logp(x^{(i)}; \\theta)) \\geq max(\\sum\\limits_{i}\\sum\\limits_{z^{(i)}} Q_i(z^{i})log\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e879b94",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "In order to prove that the given equation holds, we use the Jensen's inequality function, which states that for any convex function this $E[g(X)] >= g[E(X)]$ holds, where E is the expectation and X is the random variable.\n",
    "\n",
    "The logarithmic version of the aforementioned Jensen's equation looks like $E[\\log(X)] \\geq \\log(E[X])$.\n",
    "\n",
    "Now, we write our given expression as follows:\n",
    "\n",
    "$\n",
    "\\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\log p(x^{(i)}; \\theta) \\right) \\geq \\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{z^{(i)}} Q_i(z^{(i)}) \\log \\frac{Q_i(z^{(i)})}{p(x^{(i)}, z^{(i)}; \\theta)} \\right)\n",
    "$\n",
    "\n",
    "Then, we apply the Jensen's inequality equation to our expression:\n",
    "\n",
    "$\n",
    "\\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\log p(x^{(i)}; \\theta) \\right) \\geq \\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} E_{z^{(i)}} \\left[ \\log \\frac{Q_i(z^{(i)})}{p(x^{(i)}, z^{(i)}; \\theta)} \\right] \\right)\n",
    "$\n",
    "\n",
    "In this expression, the varibale that should be maximized is $z^{(i)}$, so we have place the maximization inside the expectation value:\n",
    "\n",
    "$\n",
    "\\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\log p(x^{(i)}; \\theta) \\right) \\geq \\frac{1}{m} \\sum_{i=1}^{m} \\max_{z^{(i)}} \\left( E_{z^{(i)}} \\left[ \\log \\frac{Q_i(z^{(i)})}{p(x^{(i)}, z^{(i)}; \\theta)} \\right] \\right)\n",
    "$\n",
    "\n",
    "It can be seen that\n",
    "$\n",
    "\\max_{z^{(i)}} \\left( E_{z^{(i)}} \\left[ \\log \\frac{Q_i(z^{(i)})}{p(x^{(i)}, z^{(i)}; \\theta)} \\right] \\right)\n",
    "$\n",
    "\n",
    "is exactly the definition of the expected log likelihood from the distribution \\(Q_i(z^{(i)})\\). Considering this, we now have:\n",
    "\n",
    "$\n",
    "\\max \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\log p(x^{(i)}; \\theta) \\right) \\geq \\frac{1}{m} \\sum_{i=1}^{m} \\max_{z^{(i)}} \\left( E_{z^{(i)}} \\left[ \\log \\frac{Q_i(z^{(i)})}{p(x^{(i)}, z^{(i)}; \\theta)} \\right] \\right)\n",
    "$\n",
    "\n",
    "So, this is the proof that the given inequality holds based on Jensen's inequality function.\n",
    "\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:46:04.779476Z",
     "start_time": "2023-12-12T17:46:04.740536Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
