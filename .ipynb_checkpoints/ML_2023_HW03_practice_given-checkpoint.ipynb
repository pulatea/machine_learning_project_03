{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1773b71c",
   "metadata": {},
   "source": [
    "# Home Assignment No. 3: Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008259a4",
   "metadata": {},
   "source": [
    "To solve this task efficiently, here are some practical suggestions:\n",
    "\n",
    "* Please include your name, surname and matriculation number in the beginning of the notebook file.\n",
    "\n",
    "* Make sure that you download data.zip file and extract it to the same folder with this notebook file.\n",
    "\n",
    "* Please contact Alp Eren SARI (alp.sari@unibe.ch) for questions and problems related to the assignment\n",
    "\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [pandas](https://pandas.pydata.org/docs/reference/index.html), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that tutorials, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, the whole life?).\n",
    "\n",
    "\n",
    "* Instead of rewriting existing code use **BUILT-IN METHODS** available in the libraries. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "\n",
    "* To complete this part of the homework, you have to write some **CODE** directly inside the specified places in the notebook **CELLS**.\n",
    "\n",
    "\n",
    "* In some problems you are asked to provide a short discussion of the results. In these cases you have to create a **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "\n",
    "* For every separate problem, you can get **INTERMEDIATE scores**, but there are **NO INTERMEDIATE scores** for separate cells.\n",
    "\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if a reviewer executes your code, the output will be the same (with all the corresponding plots) as in your uploaded notebook. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudorandomness.\n",
    "\n",
    "\n",
    "* Your code must be readable to any competent reviewer. For this purpose, try to include **necessary** (and not more) comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY**.\n",
    "\n",
    "\n",
    "* Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameter and set it to `-1` to use all the cores.\n",
    "\n",
    "\n",
    "* This assignment has $\\color{red}{\\text{(Bonus)}}$ tasks. They are optional. However, the points for them can be applied to improve the final grade for the course.\n",
    "\n",
    "\n",
    "* In the end you need to hand in a **single zip file** containing **two notebooks** (theory and practice) as well as the **html exported version** of this notebook.\n",
    "\n",
    "\n",
    "To begin let's import the essential (for this assignment) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f178f",
   "metadata": {},
   "source": [
    "## Task 1. Feature Engineering and Bagging Ensembles of Regressors [8 points]\n",
    "\n",
    "In this problem, you are to deal with [Student Stress Factors: A Comprehensive Analysis](https://www.kaggle.com/datasets/rxnach/student-stress-factors-a-comprehensive-analysis/) dataset.\n",
    "\n",
    "Your goal will be to determine the optimal parameters for two Bagging-Based Forest Ensemble **Classifiers** and compare the forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462f3c1",
   "metadata": {},
   "source": [
    "## Task 1.1. Feature Engineering [4 points]\n",
    "We are going to use the 'depression' as our target variable. Since the values of this feature are between 0 and 27, we'll convert it into binary variable by thresholding. We'll classify students with 'depression' higher than 13 as depressed (1) and students with 'depression' lower or equal to 13 as not depressed (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data = pd.read_csv('data/StressLevelDataset.csv').dropna()\n",
    "\n",
    "### BEGIN Solution\n",
    "# depression higher than 13 should be converted to 1, if it's equal to 13 or lower than 13, then it should be converted to 0\n",
    "\n",
    "# 4 points\n",
    "### END Solution\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "X = data.drop('depression', axis=1)\n",
    "y = data.depression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "data.sample(3).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca482d6",
   "metadata": {},
   "source": [
    "## Task 1.2. Bagging Ensembles of Classifiers [4 points]\n",
    "\n",
    "In this problem, you are going to compare different `RandomForestClassifier` and `ExtraTreesClassifier` models to predict a binary variable based on their **accuracy** in the test set. \n",
    "\n",
    "[Extremely Randomized Forest](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf) is another bootstraped forest with a simple tree building algorithm. Each tree node split is chosen at random w.r.t. both feature and threshold (while in random forests the split minimizes impurity).\n",
    "\n",
    "You have to do the following steps:\n",
    "\n",
    "1. For both Forests perform the [**Grid Search**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (on the train data) over the most important algorithm's parameters.\n",
    "\n",
    "2. For the optimal parameters report the train, validation, and test scores. Note that ´RandomForestClassifier´ and ´ExtraTreesClassifier´ has a score function to calculate the coefficient of determination score for a given dataset. You may use this one to compute scores.\n",
    "\n",
    "Decision Trees algorithms contain many heuristics. Most heuristics map well to objectives:\n",
    "\n",
    "* **Splitting** by information gain $\\rightarrow$ training loss\n",
    "\n",
    "* **Pruning**  $\\rightarrow$ regularization defined by the number of nodes\n",
    "\n",
    "* **Maximum depth** $\\rightarrow$ constraint on the function space\n",
    "\n",
    "* **The minimum number of samples** required to be at a **leaf node** $\\rightarrow$ smoothing the model, especially in regression\n",
    "\n",
    "So, we will tune the following hyper-parameters: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`.\n",
    "\n",
    "\n",
    "The Extra-(Randomized)-Trees might be a bit worse than Random Forests when there are many noisy features. From the article [Extremely Randomized Forest](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf): \n",
    "> The analysis of the algorithm and the determination of the optimal value of K on several test problem variants have shown that the value is in principle dependent on problem specifics, in particular the proportion of irrelevant attributes. [...] The bias/variance analysis has shown that Extra-Trees work by decreasing variance while at the same time increasing bias. [...] _When the randomization increases above the optimal level, variance decreases slightly, while bias increases often significantly_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e586249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae82d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameters_tuning(X_train, y_train, X_test, y_test, classifier, parameters, scoring, cv):\n",
    "    print(f'Tuning hyper-parameters for {classifier.__class__.__name__}...\\n')\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    # use GridSearchCV for the given classifier, parameters and data\n",
    "    \n",
    "    \n",
    "    # 3 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    print(f'Best hyper-parameters: {best_parameters}\\n')\n",
    "\n",
    "    print('Train: {:.3f}'.format(train_score))\n",
    "    \n",
    "    print('Valid: {:.3f}'.format(val_score))\n",
    "    \n",
    "    print('Test: {:.3f}\\n'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "#declare parameters, random_forest and extra_trees classifiers\n",
    "\n",
    "# 1 points\n",
    "\n",
    "### END Solution\n",
    "\n",
    "hyper_parameters_tuning(np.array(X_train), np.array(y_train),\n",
    "                        np.array(X_test), np.array(y_test),\n",
    "                        random_forest,\n",
    "                        parameters,\n",
    "                        scoring='accuracy', cv=5)\n",
    "\n",
    "hyper_parameters_tuning(np.array(X_train), np.array(y_train),\n",
    "                        np.array(X_test), np.array(y_test),\n",
    "                        extra_trees,\n",
    "                        parameters,\n",
    "                        scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf03091",
   "metadata": {},
   "source": [
    "## Task 2. AdaBoost [26 points]\n",
    "\n",
    "> Boosting Machines (BM) is a family of widely popular and effective methods for classification and regression. The main idea behind BMs is that **combining weak learners**, that perform slightly better than random, can result in **strong learning models**.\n",
    "\n",
    "> AdaBoost utilizes the greedy training approach: first, we train the weak learners (they are later called `base_classifiers`) on the whole dataset, and in the next iterations, we train the model on the samples, on which the previous models have performed poorly. This behavior is achieved by reweighting the training samples at each step. \n",
    "\n",
    "#### The task:\n",
    "\n",
    "In this exercise, you are asked to implement one of the earlier variants of BMs - **AdaBoost**, a modified version of it and compare these to the already existing `sklearn` implementation. The key steps are:\n",
    "\n",
    "* Copy the the solution from task 1.1 so that `depression` variable is binarized.  \n",
    "\n",
    "* Complete the `ada_boost_alpha`, `ada_boost_distribution` and `ada_boost_distribution_modify` functions\n",
    "\n",
    "* Complete the `.fit` method of `Boosting` class\n",
    "\n",
    "* Complete the `.predict` method of `Boosting` class\n",
    "\n",
    "##### Criteria:\n",
    "\n",
    "The decision boundary of the final implementation should look reasonably identical to the model from `sklearn`, and should achieve accuracy close to `scikit`:\n",
    "\n",
    "$$ |\\text{your_accuracy} - \\text{sklearn_accuracy}| \\leq 0.005\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7264fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b55172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data = pd.read_csv('data/StressLevelDataset.csv').dropna()\n",
    "\n",
    "### BEGIN Solution\n",
    "# depression higher than 13 should be converted to 1, if it's equal to 13 or lower than 13, then it should be converted to 0\n",
    "\n",
    "\n",
    "### END Solution\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "# we'll only use living_conditions and stress_level features\n",
    "X = data[['living_conditions', 'stress_level']].to_numpy()\n",
    "y = data.depression.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the dataset\n",
    "\n",
    "# X, y = make_moons(n_samples=1000, noise=0.3, random_state=0)\n",
    "X = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, keepdims=True)\n",
    "X += 0.5 * np.random.randn(*X.shape) \n",
    "# for convenience convert labels from {0, 1} to {-1, 1}\n",
    "y[y == 0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 30),\n",
    "                     np.linspace(y_min, y_max, 30))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='.', c=y_test, cmap=cm_bright);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d292ed30",
   "metadata": {},
   "source": [
    "**Base classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398b666",
   "metadata": {},
   "source": [
    "* `ada_boost_alpha` - function, which calculates the weights of the linear combination of the classifiers\n",
    "\n",
    "* `ada_boost_distribution` - function, which calculates sample weights\n",
    "\n",
    "* `ada_boost_distribution` - a modified function, which calculates sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost_alpha(y, y_pred_k, distribution):\n",
    "    # y_pred_k is a prediction of the k-th base classifier\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    # 2 points\n",
    "    \n",
    "    ### END Solution\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eff325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost_distribution(y, y_pred_k, distribution, alpha_k):\n",
    "    # y_pred_k is a prediction of the k-th base classifier\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    # 2 points\n",
    "    \n",
    "    ### END Solution\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98e52",
   "metadata": {},
   "source": [
    "* Now modify the `ada_boost_distribution` function so that the expression `y * y_pred_k` will be scaled between `-2t+1` and 1 for a t value which should be equal to `0.5`. After that, we'll also use a version of adaboost using this distrbution along with the original one. Hint: `y * y_pred_k` is equal to either -1 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a989ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost_distribution_modify(y, y_pred_k, distribution, alpha_k, t=0.5):\n",
    "    # y_pred_k is a prediction of the k-th base classifier\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    \n",
    "    # 4 points\n",
    "    \n",
    "    ### END Solution\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b07f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost(object):\n",
    "    \"\"\"\n",
    "    Generic class for construction of boosting models\n",
    "    \n",
    "    :param n_estimators: int, number of estimators (number of boosting rounds)\n",
    "    :param base_classifier: callable, a function that creates a weak estimator. Weak estimator should support sample_weight argument\n",
    "    :param get_alpha: callable, a function, that calculates new alpha given current distribution, prediction of the t-th base estimator,\n",
    "                      boosting prediction at step (t-1) and actual labels\n",
    "    :param get_distribution: callable, a function, that calculates samples weights given current distribution, prediction, alphas and actual labels\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_classifier=None,\n",
    "                 n_estimators=50,\n",
    "                 get_alpha=ada_boost_alpha,\n",
    "                 update_distribution=ada_boost_distribution):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_classifier = base_classifier\n",
    "        self.get_alpha = get_alpha\n",
    "        self.update_distribution = update_distribution\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        distribution = np.ones(n_samples, dtype=float) / n_samples\n",
    "        self.classifiers = []\n",
    "        self.alphas = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # create a new classifier\n",
    "            \n",
    "            self.classifiers.append(self.base_classifier())     \n",
    "            self.classifiers[-1].fit(X, y, sample_weight=distribution)\n",
    "\n",
    "            ### BEGIN Solution\n",
    "            \n",
    "            # make a prediction  \n",
    "            \n",
    "            # update alphas, append new alpha to self.alphas\n",
    "            \n",
    "            # update distribution and normalize\n",
    "            \n",
    "            # 6 points\n",
    "            \n",
    "            ### END Solution\n",
    "    \n",
    "    def predict(self, X):\n",
    "        final_predictions = np.zeros(X.shape[0])\n",
    "    \n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        # get the weighted votes of the classifiers\n",
    "        \n",
    "        # 4 points\n",
    "        \n",
    "        ### END Solution\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "max_depth = 5\n",
    "\n",
    "# sklearn\n",
    "ada_boost_sklearn = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n",
    "                                       algorithm=\"SAMME\",\n",
    "                                       n_estimators=n_estimators)\n",
    "\n",
    "ada_boost_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# custom\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "# get base function for the Adaboost class we implemented\n",
    "\n",
    "# declare adaboost classifier\n",
    "\n",
    "# fit the classifier to the data\n",
    "\n",
    "# 4 points\n",
    "\n",
    "### END Solution\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "# declare modified adaboost classifier\n",
    "\n",
    "# fit the classifier to the data\n",
    "\n",
    "# 4 points\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ada_boost_sklearn, ada_boost, ada_boost_mod]\n",
    "names = ['ada_boost_sklearn', 'ada_boost', 'ada_boost_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3558d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ensemble classifier\n",
    "\n",
    "plt.figure(figsize=(23, 7))\n",
    "\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    prediction = classifier.predict(X_test)\n",
    "\n",
    "    # put the result into a color plot\n",
    "    \n",
    "    ax = plt.subplot(1, len(classifiers), i + 1)\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, alpha=0.5)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title('accuracy {}: {:.3f}'.format(names[i], (prediction == y_test).sum() * 1. / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ea79",
   "metadata": {},
   "source": [
    "## Task 3. Gradient Boosting and Feature Selection [15 points]\n",
    "\n",
    "**Gradient Boosting Machines** (GBM) are historical and logical continuations of the first boosting algorithms. In a way, one can consider AdaBoost as another variant of GBMs. These are effective tools widely used in industry, research, and various machine learning competitions. \n",
    "\n",
    "In this task, we offer to focus on one variant of GBM called [**XGBoost**](https://github.com/dmlc/xgboost/tree/master/python-package). The dataset that is going to be used is   [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "You will need to construct an **XGBoost** classification model, train it, measure the training time and compare it to **Random Forest**. Afterward, compare the models' feature importances.\n",
    "\n",
    "**The task:**\n",
    "\n",
    "* Copy the the solution from task 1.1 so that `depression` variable is binarized.  \n",
    "\n",
    "* Train the XGBoost classifier on the provided dataset\n",
    "\n",
    "  * Measure the training time\n",
    "  \n",
    "  * Measure the precision/recall on the test set\n",
    "\n",
    "* Train Random Forest classifier and compare it to XGBoost\n",
    "\n",
    "* Compare the feature importances of the trained XGBoost and Random Forest Classifiers. Why do you think they are different? Explain.\n",
    "\n",
    "**HINT**: in order to measure the training time you can use [**timeit** cell magic](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) or [**time**](https://docs.python.org/3/library/time.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ad710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data = pd.read_csv('data/StressLevelDataset.csv').dropna()\n",
    "\n",
    "### BEGIN implementation\n",
    "# depression higher than 13 should be converted to 1, if it's equal to 13 or lower than 13, then it should be converted to 0\n",
    "\n",
    "## END implementation\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "cols = list(data.drop('depression', axis=1))\n",
    "\n",
    "X = data.drop('depression', axis=1)\n",
    "y = data.depression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3332ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5066b0a",
   "metadata": {},
   "source": [
    "Implement and fit XGBClassifier and measure the convergence time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c231a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "# declare xgb classifier\n",
    "\n",
    "# fit the classifier to data and print the training time\n",
    "\n",
    "# measure precision/recall and print the results\n",
    "\n",
    "\n",
    "# 4 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c06c6f",
   "metadata": {},
   "source": [
    "Implement and fit RandomForestClassifier and measure the convergence time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare random forest classifier\n",
    "\n",
    "# fit the classifier to data and print the training time\n",
    "\n",
    "# measure precision/recall and print the results\n",
    "\n",
    "# 4 points\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eff9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1b99b",
   "metadata": {},
   "source": [
    "Time to display the importance of the weights for XGBClassifier and RandomForestClassifier and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def highlight_zeros(s):\n",
    "    is_max = s == 0\n",
    "    \n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "display(pd.DataFrame(np.vstack((xgb_classifier.feature_importances_, random_forest.feature_importances_)),\n",
    "                     index=['XGB Classifier', 'Random Forest Classifier'],\n",
    "                     columns=cols).style.apply(highlight_zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3a94e",
   "metadata": {},
   "source": [
    "**Are the feature importances calculated by the previously trained XGBoost and Random Forest Classifiers different or the same, why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330af9cd",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your Solution: }}$\n",
    "\n",
    "-----------------\n",
    "\n",
    "$\\color{red}{\\textbf{7 points}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a2e06",
   "metadata": {},
   "source": [
    "## Task 4. $\\color{red}{\\text{(Bonus)}}$. KMeans and Gaussian Mixture Algorithms [8 + 8 = 16 points]\n",
    "\n",
    "Explore **KMeans** and **Gaussian Mixture** clustering algorithms from `sklearn`.\n",
    "\n",
    "To evaluate the performance, use **two** metrics: `silhouette score` and `mutual information`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78b6b0",
   "metadata": {},
   "source": [
    "## Task 4.1. $\\color{red}{\\text{(Bonus)}}$. Grid Search [8 points]\n",
    "\n",
    "For each algorithm **your task** is to find the parameters leading to the best performance for `n_clusters=true_number_of_clusters`.\n",
    "\n",
    "Apply the algorithm with the best parameters with the true number of clusters and at least two **other settings** for the number of clusters: a smaller and a larger number than the true one.\n",
    "\n",
    "**NOTE:** `sklearn.model_selection.GridSearchCV` does not support clustering methods. Hence you are supposed to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac304c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points, labels = np.loadtxt('data/clustering.txt'), np.loadtxt('data/clustering_labels.txt')\n",
    "labels = labels.astype(int)\n",
    "print(\"True number of clusters is {}\".format(np.max(labels)))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(points[:, 0], points[:, 1], c=labels, cmap=cm.jet, alpha=0.8, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5435185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score, mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40609f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clasterizator, param_grid, metric, points, labels):\n",
    "    ### BEGIN Solution\n",
    "    # iterate over given param_grid and calculate the silhouette and mutual info scores\n",
    "    \n",
    "    # 8 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(clasterizator, param_grid, metrics, points, labels):\n",
    "    for metric in metrics:\n",
    "        best_score, best_parameters = grid_search(clasterizator, param_grid, metric, points, labels)\n",
    "        print('{} score: {:.3f} via parameters: {}'.format(metric, best_score, best_parameters))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7af6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226c40",
   "metadata": {},
   "source": [
    "## Task 4.2. $\\color{red}{\\text{(Bonus)}}$. K-Mean and Gaussian Mixtures Algorithms [8 points]\n",
    "\n",
    "* In this part, you are supposed to iterate over different cluster numbers, and do the grid search for each cluster number with `KMeans` and `GaussianMixture`. Moreover, print the best `silhouette` and `mutual_info` for each cluster number for `KMeans` and `GaussianMixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f85fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_numbers = [10, 15, 20]\n",
    "scores = ['silhouette', 'mutual_info']\n",
    "\n",
    "### BEGIN Solution\n",
    "# iterate over clusters_numbers and do grid search and print best score for each cluster number choice\n",
    "\n",
    "\n",
    "# 3 points\n",
    "    \n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b64df9",
   "metadata": {},
   "source": [
    "Apply the algorithm with the **best parameters on silhouette score** for\n",
    "\n",
    "* Cluster number of 10, 15 and 20. Use the best parameters for each cluster number using the `grid_search` you implemented\n",
    "\n",
    "* Visualize the results for each case (6 cases in total: 10, 15, 20 clusters with KMeans and GaussianMixture) using the `plot_results` function we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(ax, clasterizator_name, n_clusters, pred_labels):\n",
    "    ax.scatter(points[:,0], points[:,1], c=pred_labels, cmap=cm.jet, alpha=0.8, edgecolor='k')\n",
    "    ax.set_title('{} with {} clusters'.format(clasterizator_name, n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(24, 12))\n",
    "clusters_numbers = [10, 15, 20]\n",
    "\n",
    "### BEGIN Solution\n",
    "# iterate over cluster numbers do the prediction with best KMeans and GaussianMixture model and visualize the results\n",
    "\n",
    "\n",
    "# 5 points\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc71d92",
   "metadata": {},
   "source": [
    "## Task 5. Clustering [24 + 16 $\\color{red}{\\text{(Bonus)}}$ + 10 = 50 points]\n",
    "\n",
    "In this task you will:\n",
    "\n",
    "* implement **k-means** clustering;\n",
    "\n",
    "* train a **Mixture of Gaussian** model by implementing the **EM** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ead22c",
   "metadata": {},
   "source": [
    "Let's generate some test data for you to test your implementation of the k-means and EM algorithm.\n",
    "\n",
    "Note that we generate gaussian blobs with non-isotropic covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c483c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdab72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples of 4 different gaussians\n",
    "\n",
    "rv1 = multivariate_normal.rvs([3.0, -3.], [[3.0, 1.2], [1.7, 0.9]], size=400, random_state=0)\n",
    "rv2 = multivariate_normal.rvs([4.0, 1.25], [[1.45, 1.15], [2.2, 2.75]], size=500, random_state=0)\n",
    "rv3 = multivariate_normal.rvs([-3.5, -4.0], [[0.1, 1.0], [3.0, 3.]], size=600, random_state=0)\n",
    "rv4 = multivariate_normal.rvs([-3., 2.], [[3, 1.1], [-0.9, 1]], size=700, random_state=0)\n",
    "\n",
    "# concatenate the samples and create corresponding labels\n",
    "\n",
    "X = np.concatenate([rv1, rv2, rv3, rv4], 0)\n",
    "y = np.array([0]*400 + [1]*500 + [2]*600 + [3]*700)\n",
    "\n",
    "# plot the test data\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.jet, alpha=0.8, edgecolor='k')\n",
    "plt.title(\"Test Blobs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b4a9d",
   "metadata": {},
   "source": [
    "## Task 5.1. A modified K-Means [24 points]\n",
    "\n",
    "You will need to implement the a slighlt modified **k-means** clustering algorithm and test it on the generated test data. Initialize the cluster centers from a Gaussian distribution with a mean 0 and covariance matrix $\\sigma I$ where $\\sigma = 0.2 \\times max(|X|)$ if you flatten $X$; hence, $\\sigma$ is a scallar. Morever, use L-infinity norm to assign data points to clusters instead of L2 norm which is $max(abs(x))$ for a vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansCustom(object):\n",
    "    def __init__(self, n_clusters, max_iter=100):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.cluster_centers_ = None\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        # 1st step: chose n_clusters random rows of X as initial cluster centers     \n",
    "\n",
    "        # 2nd step: update the cluster assignment\n",
    "\n",
    "        # 3rd step: check for convergence\n",
    "          \n",
    "        # 4th step: update the cluster centers based on the new assignment\n",
    "           \n",
    "        \n",
    "        # 6 + 7 + 4 + 7 = 24 points\n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cbd6cc",
   "metadata": {},
   "source": [
    "Let's test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, clasterizator):\n",
    "    clasterizator.fit(X)\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=clasterizator.labels_, cmap=cm.jet)\n",
    "    plt.scatter(clasterizator.cluster_centers_[:, 0], clasterizator.cluster_centers_[:, 1], c='r', marker='x')\n",
    "    \n",
    "    plt.title(\"Predicted clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85419bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X, KMeansCustom(n_clusters=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770782af",
   "metadata": {},
   "source": [
    "## Task 5.2 $\\color{red}{\\text{(Bonus)}}$. Expectation Maximization [16 points]\n",
    "\n",
    "In this task, you will need to implement the EM algorithm for the Mixture of Gaussian models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fa46d",
   "metadata": {},
   "source": [
    "* Let's implement the **E-step** of the EM algorithm.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "* Use `scipy.stats.multivariate_normal.pdf()` to compute the pdf of a gaussian with the current parameters.\n",
    "\n",
    "* $\\omega = \\mathbb{P}(z = j | x; \\mu, \\sigma, \\phi)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088049ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, mu, sigma, phi):\n",
    "    ### BEGIN Solution\n",
    "    # implement e-step of EM algorithm\n",
    "    \n",
    "    \n",
    "    # 2 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95b2c0",
   "metadata": {},
   "source": [
    "* Let's update all the model parameters as per the **M-step** of the EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(w, X, mu, sigma, phi, n_gaussians):\n",
    "    ### BEGIN Solution\n",
    "    # implement m-step of EM algorithm\n",
    "    \n",
    "    \n",
    "    # 6 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return phi, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9364c",
   "metadata": {},
   "source": [
    "* Let's implement the **log-likelihood** of the data under the current model to check for convergence of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, mu, sigma, phi):\n",
    "    ### BEGIN Solution\n",
    "    # implement log-likelihood \n",
    "    \n",
    "    \n",
    "    # 2 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return log_likelihood_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1d8ee",
   "metadata": {},
   "source": [
    "Finally, fit the Mixture of Gaussians model using the **EM algorithm**.\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "* $\\phi$ is a vector of probabilities for the latent variables $z$ of shape `[n_gaussians]`\n",
    "\n",
    "* $\\mu$ is a marix of mean vectors of shape `[n_gaussians, num_features]` \n",
    "\n",
    "* $\\sigma$ is a list of length `[n_gaussians]` of covariance matrices each of shape `[num_features, num_features]`\n",
    "\n",
    "* $w$ is a vector of weights for the `[n_gaussians]` gaussians per example of shape `[n_gaussians, k]` (a result of the E-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(X, n_gaussians, max_iter=20):\n",
    "    # initialization\n",
    "    \n",
    "    mu = None\n",
    "    sigma = [np.eye(X.shape[1]) for i in range(n_gaussians)]\n",
    "    phi = np.ones([n_gaussians,]) / n_gaussians\n",
    "    \n",
    "    log_likelihood_previous = float('inf')\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    # Use k-means to initialize the means of the gaussians\n",
    "    \n",
    "    \n",
    "    # E-Step\n",
    "        \n",
    "        \n",
    "    # M-step\n",
    "    \n",
    "    # check convergence  \n",
    "\n",
    "    \n",
    "    # compute final assignment\n",
    "    \n",
    "    # 6 points\n",
    "    \n",
    "    ### END Solution\n",
    "    \n",
    "    return phi, mu, sigma, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b4c71",
   "metadata": {},
   "source": [
    "Let's test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da816073",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi, mu, sigma, w = EM(X, n_gaussians=4)\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.argmax(w, 1), cmap=cm.jet)\n",
    "plt.scatter(mu[:, 0], mu[:, 1], c='r', marker='x')\n",
    "plt.title(\"Predicted clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42514ebc",
   "metadata": {},
   "source": [
    "## Task 5.3.1 K-Means for Image Compression [6 points]\n",
    "\n",
    "You will now use **k-means to cluster pixel values of an image** and use the cluster assignments as a way to quantize/compress the color-space of the image.\n",
    "\n",
    "Feel free to test the algorithm on your own images and with different number of clusters.\n",
    "* Let's implement the KMeans function for our images which applies K-Means clustering in pixel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_colors(image, n_colors, clasterizator):\n",
    "    ### BEGIN Solution\n",
    "    # take the image and apply kmeans so that final image will only have n_colors pixels\n",
    "   \n",
    "    \n",
    "    # 6 points\n",
    "    \n",
    "    ### END Solution\n",
    "\n",
    "    return image_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f43eb4",
   "metadata": {},
   "source": [
    "## Task 5.3.2 K-Means for Image Compression with Different Number of Pixels [4 points]\n",
    "\n",
    "Let's test your implementation and try to quantize the given image into 3, 6, 9 and 12 colors in HSV color space. For each quantization case:\n",
    "\n",
    "* Take the image which is already in HSV color space and then perform the quantization\n",
    "\n",
    "* Calculate mean squared error between the original image and the quantized image.\n",
    "\n",
    "* Convert the quantized image to RGB colorspace for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35999a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, nrows=1, figsize=(24, 12))\n",
    "\n",
    "# load and show test image\n",
    "image = Image.open('data/ship_image.jpg').resize((512, 512))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"Original image\")\n",
    "\n",
    "image = np.array(image.convert('HSV'))\n",
    "image_losses = []\n",
    "\n",
    "for i in range(4):\n",
    "    n_colors = (i + 1) * 3\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    # compute quantized image\n",
    "\n",
    "\n",
    "    # show the quantized image\n",
    "    \n",
    "    # 4 points\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a1dee",
   "metadata": {},
   "source": [
    "Let's draw the plot of the MSE vs number colors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot([3*(i+1) for i in range(4)], image_losses)\n",
    "plt.xlabel('Number of Colors')\n",
    "plt.ylabel('Mean Square Quantization Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff0838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
